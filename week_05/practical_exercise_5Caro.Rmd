---
title: "practical_exercise_5, Methods 3, 2021, autumn semester"
author: 'Caroline Hommel'
date: "[FILL IN THE DATE]"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("/Users/CarolineHommel/Desktop/Uni/Semester tres/Methods 3/methods git/github_methods_3/week_05")
pacman::p_load('tidyverse', 'lme4', 'stats', 'boot', 'multcomp', 'ggplot2', 'dplyr', 'mgcv', 'tidymv' )
```

# Exercises and objectives
The objectives of the exercises of this assignment are based on: https://doi.org/10.1016/j.concog.2019.03.007  

4) Download and organise the data from experiment 1  
5) Use log-likelihood ratio tests to evaluate logistic regression models  
6) Test linear hypotheses  
7) Estimate psychometric functions for the Perceptual Awareness Scale and evaluate them  

# EXERCISE 4 - Download and organise the data from experiment 1

Go to https://osf.io/ecxsj/files/ and download the files associated with Experiment 1 (there should be 29).  
The data is associated with Experiment 1 of the article at the following DOI https://doi.org/10.1016/j.concog.2019.03.007 


1) Put the data from all subjects into a single data frame - note that some of the subjects do not have the _seed_ variable. For these subjects, add this variable and make in _NA_ for all observations. (The _seed_ variable will not be part of the analysis and is not an experimental variable)  
  i. Factorise the variables that need factorising 
```{r -4.1 i loading data}
# Making a data frame with the data from experiment 1
data.list <- list.files(path = "experiment_1",
                    pattern = ".csv",
                    full.names = T)

df <- data.frame() # Creating empty data frame

for (i in 1:length(data.list)){
  new_dat <- read.table(data.list[i], sep = ",", header = TRUE, dec = ',', stringsAsFactors = T )
  new_dat$seed <- NA
  df <- rbind(new_dat, df)
}

df$subject <- as.factor(df$subject)
df$pas <- as.factor(df$pas)
```

  ii. Remove the practice trials from the dataset (see the _trial.type_ variable)  
```{r -4.1 ii}
df <- df %>% 
  filter(df$trial.type == 'experiment')
```

  iii. Create a _correct_ variable  
```{r -4.1 iii}
#creating the 'correct' variable and changing even to being an e and odd to being an o
df$correct <- ifelse((df$target.type == 'even' & df$obj.resp == 'e') | (df$target.type == 'odd' & df$obj.resp == 'o'), 1, 0)
```

  iv. Describe how the _target.contrast_ and _target.frames_ variables differ compared to the data from part 1 of this assignment  

Target.contrast is the same through all data points on 0.1, while it in the 1st part of this assignment was adjusted for each participant. 
As for target.frames it is how long the stimuli object is presented for 1 frame is 11.8 seconds, 2 frames is 23.6 seconds.
Target.frames as a variable was not included in the first part of the assignment. 


# EXERCISE 5 - Use log-likelihood ratio tests to evaluate logistic regression models

1) Do logistic regression - _correct_ as the dependent variable and _target.frames_ as the independent variable. (Make sure that you understand what _target.frames_ encode). Create two models - a pooled model and a partial-pooling model. The partial-pooling model should include a subject-specific intercept.  
```{r -5.1}

pooled <- glm(correct ~ target.frames, data = df, family = 'binomial')
partial_pool <- glmer(correct ~ target.frames + (1|subject), data = df, family = 'binomial')
```

  i. the likelihood-function for logistic regression is: $L(p)={\displaystyle\prod_{i=1}^Np^{y_i}(1-p)^{(1-y_i)}}$ (Remember the probability mass function for the Bernoulli Distribution). Create a function that calculates the likelihood.
```{r -5.1 i}
likelihood <- function(model){
  yhat <- model$fitted.values
  y <- model$y
  prod((yhat^y)*(1-yhat)^(1-y))
}
```
  
  ii. the log-likelihood-function for logistic regression is: $l(p) = {\displaystyle\sum_{i=1}^N}[y_i\ln{p}+(1-y_i)\ln{(1-p)}$. Create a function that calculates the log-likelihood 
```{r -5.1 ii}
log_likelihood <- function(model){
  yhat <- fitted.values(model)
  y <- df$correct
  sum(y*log(yhat)+(1-y)*log(1-yhat))
}
```
  
  iii. apply both functions to the pooling model you just created. Make sure that the log-likelihood matches what is returned from the _logLik_ function for the pooled model. Does the likelihood-function return a value that is surprising? Why is the log-likelihood preferable when working with computers with limited precision? 
```{r -5.1 iii}
likelihood(pooled)
log_likelihood(pooled)
logLik(pooled)
```
Because all the probabilities are independent they can be multiplied together and this final number is so close to 0, but theoretically it is never 0. The likelihood function returns 0 because computers with limited precision cannot return the specific number since it is so small. Yhat can never be 0 it is on the logarithm scale and the log(0) is not defined. 
Log on a number between 0-1 will return a negative value that makes it possible for us to compare models with each other on a relative scale, since likelihood by it's own will return 0 and comparing 0 with 0 is quite difficult. 


  iv. now show that the log-likelihood is a little off when applied to the partial pooling model - (the likelihood function is different for the multilevel function - see section 2.1 of https://www.researchgate.net/profile/Douglas-Bates/publication/2753537_Computational_Methods_for_Multilevel_Modelling/links/00b4953b4108d73427000000/Computational-Methods-for-Multilevel-Modelling.pdf if you are interested)  
```{r -5.1 iv}
log_likelihood(partial_pool)
logLik(partial_pool)
```
It's off because the log likelihood function is different for the multilevel function. LogLik can take their differences into consideration which is why this number is turns out different. 


2) Use log-likelihood ratio tests to argue for the addition of predictor variables, start from the null model, `glm(correct ~ 1, 'binomial', data)`, then add subject-level intercepts, then add a group-level effect of _target.frames_ and finally add subject-level slopes for _target.frames_. Also assess whether or not a correlation between the subject-level slopes and the subject-level intercepts should be included.
```{r -5.2}
null <- glm(correct ~ 1, family = 'binomial', data = df)
m2 <- glmer(correct ~ 1 + (1|subject), data = df, family = 'binomial')
m3 <- glmer(correct ~ target.frames + (1|subject), data = df, family = 'binomial')
m4 <- glmer(correct ~ target.frames + (1 + target.frames||subject), data = df, family = 'binomial')
m5 <- glmer(correct ~ target.frames + (1 + target.frames|subject), data = df, family = 'binomial')
```

```{r -5.2}
anova(m2, m3, m4, m5, null)
```
We chose our m5 model:

correct ~ target.frames + (1 + target.frames | subject)

We chose this based on the fact that comparing m4 and m5 we see that including correlation between subject-level slopes and the subject-level intercepts improves the model. When using the anova-function the null hypothesis states that the addition of the parameters between the models is only noise, but since m5 both has the best logLik (-10449) and a significant p-value (p<0) it means that these randoms effects are not only noise, but that they actually 'explain' something in the model. 


  i. write a short methods section and a results section where you indicate which model you chose and the statistics relevant for that choice. 
Starting from the null model we slowly increased the complexity of models by adding more predictions and random effects. Then the models were compared by running an ANOVA test on all of the models and by that we found the best model. 

  
Include a plot of the estimated group-level function with `xlim=c(0, 8)` that includes the estimated subject-specific functions.
```{r -5.2 i }
df %>% ggplot() +
  geom_smooth(aes(x = target.frames, y = fitted.values(pooled), color = 'Pooling'))+
  geom_smooth(aes(x = target.frames, y = fitted.values(m5), color = 'Partial Pooling')) + 
  facet_wrap(~subject) +
  xlab('Target Frame') +
  ylab('Fitted values') +
  xlim(0, 6) +
  theme_minimal()+
  ggtitle('Estimated group level functions with subject-specific functions')

```

  ii. also include in the results section whether the fit didn't look good for any of the subjects. If so, identify those subjects in the report, and judge (no statistical test) whether their performance (accuracy) differed from that of the other subjects. Was their performance better than chance? (Use a statistical test this time) (50 %)  

```{r -5.2 ii}
subject24 <- df %>%  filter(subject == '24')
t.test(subject24$correct, mu=0.5)


```
Subject number 24 looks a little off, especially when looking at it's partial pooling. I calculated a t-test which shows the mean of it's corrects (56.7%). It's significant above chance level, however since it is 'only' 56% it means that subject 24 could almost just have guessed throughout the whole experiment.

3) Now add _pas_ to the group-level effects - if a log-likelihood ratio test justifies this, also add the interaction between _pas_ and _target.frames_ and check whether a log-likelihood ratio test justifies this  
```{r -5.3}
pas_model <- glmer(correct ~ pas + target.frames + (1 + target.frames|subject), data = df, family = 'binomial')
interact <- glmer(correct ~ pas*target.frames + (1 + target.frames|subject), data = df, family = 'binomial')

anova(pas_model, interact, m5)
```

  i. if your model doesn't converge, try a different optimizer  
It converges. 

  ii. plot the estimated group-level functions over `xlim=c(0, 8)` for each of the four PAS-ratings - add this plot to your report (see: 5.2.i) and add a description of your chosen model. Describe how _pas_ affects accuracy together with target duration if at all. 
```{r -5.3 ii}
grouplevel <- glm(correct ~ target.frames * pas, family = 'binomial', data = df)

df %>% ggplot(aes(x = target.frames, y = fitted.values(grouplevel), color = pas)) +
  geom_point(x = df$target.frames, y = fitted.values(interact)) + 
  geom_line() + 
  facet_wrap(~ pas) +
  ggtitle('Estimated accuracy based on pas in group level functions') +
  xlab('Target Frame') +
  ylab('Fitted values') +
  xlim(0, 8) +
  theme_minimal() 
```

Also comment on the estimated functions' behavior at target.frame=0 - is that behavior reasonable?  
```{r -5.3 ii}
#accessing the intercept
inv.logit(interact@beta[1])
```
When target.frame = 0 taking the intercept gives us the value of 46.9% which is a value around 50%. This makes sense, since it's 50/50 whether or not you would choose 'even' or 'odd' if you are not presented with any target.frames on the screen during the experiment. 


# EXERCISE 6 - Test linear hypotheses

In this section we are going to test different hypotheses. We assume that we have already proved that more objective evidence (longer duration of stimuli) is sufficient to increase accuracy in and of itself and that more subjective evidence (higher PAS ratings) is also sufficient to increase accuracy in and of itself.  
We want to test a hypothesis for each of the three neighboring differences in PAS, i.e. the difference between 2 and 1, the difference between 3 and 2 and the difference between 4 and 3. More specifically, we want to test the hypothesis that accuracy increases faster with objective evidence if subjective evidence is higher at the same time, i.e. we want to test for an interaction.  

1) Fit a model based on the following formula: `correct ~ pas * target.frames + (target.frames | subject))`
    i. First, use `summary` (yes, you are allowed to!) to argue that accuracy increases faster with objective evidence for PAS 2 than for PAS 1. 
```{r -6.1}
summary(interact)
```
Since the coefficient is positive (0.44) we can see that there is an increase in accuracy with objective evidence from pas1 to pas2.


2) `summary` won't allow you to test whether accuracy increases faster with objective evidence for PAS 3 than for PAS 2 (unless you use `relevel`, which you are not allowed to in this exercise). Instead, we'll be using the function `glht` from the `multcomp` package
    i. To redo the test in 6.1.i, you can create a _contrast_ vector. This vector will have the length of the number of estimated group-level effects and any specific contrast you can think of can be specified using this. For redoing the test from 6.1.i, the code snippet below will do

  ii. Now test the hypothesis that accuracy increases faster with objective evidence for PAS 3 than for PAS 2.
```{r -6.2 ii}
## intercepts between PAS 2 and PAS 3
contrast.vector2 <- matrix(c(0, 0, 0, 0, 0, -1, 1, 0), nrow=1)
gh2 <- glht(interact, contrast.vector2)
print(summary(gh2))
```
Accuracy increases from pas2 to pas3, this we see on the positive coefficient (0.30) and it is significant. 

  iii. Also test the hypothesis that accuracy increases faster with objective evidence for PAS 4 than for PAS 3
```{r -6.2 iii}
## intercepts between PAS 2 and PAS 3
contrast.vector3 <- matrix(c(0, 0, 0, 0, 0, 0, -1, 1), nrow=1)
gh3 <- glht(interact, contrast.vector3)
print(summary(gh3))
```
Accuracy increases from pas3 to pas4, this we see on the positive coefficient (0.01) but there is not a significant difference. 

3) Finally, test that whether the difference between PAS 2 and 1 (tested in 6.1.i) is greater than the difference between PAS 4 and 3 (tested in 6.2.iii)
```{r -6.3}
## testing whether PAS 2 is different from PAS 1
contrast.vector <- matrix(c(0, 0, 0, 0, 0, 1, 0, 0), nrow=1)
gh <- glht(interact, contrast.vector)

# difference of differences
K <- contrast.vector - contrast.vector3
#First one is from pas1 to pas2 - the second is from pas3 to pas4

t <- glht(interact, linfct = K)
summary(t)
```
There is a bigger difference between pas 2 and pas 1 compared to the difference between pas 3 and pas 4. It is significantly different. We see this in the positive coefficient (0.43). Significant difference between the differences of the two groups. 

# EXERCISE 7 - Estimate psychometric functions for the Perceptual Awareness Scale and evaluate them  

We saw in 5.3 that the estimated functions went below chance at a target duration of 0 frames (0 ms). This does not seem reasonable, so we will be trying a different approach for fitting here.  
We will fit the following function that results in a sigmoid, $f(x) = a + \frac {b - a} {1 + e^{\frac {c-x} {d}}}$  
It has four parameters: _a_, which can be interpreted as the minimum accuracy level, _b_, which can be interpreted as the maximum accuracy level, _c_, which can be interpreted as the so-called inflexion point, i.e. where the derivative of the sigmoid reaches its maximum and _d_, which can be interpreted as the steepness at the inflexion point. (When _d_ goes towards infinity, the slope goes towards a straight line, and when it goes towards 0, the slope goes towards a step function).  

We can define a function of a residual sum of squares as below

```{r -7}
RSS <- function(dataset, par)
{
    ## "data set" should be a data.frame containing the variables x (target.frames)
    ## and y (correct)
    
    ## "par" are our four parameters (a numeric vector) 
    a = par[1]
    b = par[2]
    c = par[3]
    d = par[4]
    
    x <- dataset$x
    y <- dataset$y
    
    y.hat <- a + ((b-a)/(1 + exp((c-x)/d)))
    
    RSS <- sum((y - y.hat)^2)
    return(RSS)
}
```

1) Now, we will fit the sigmoid for the four PAS ratings for Subject 7
    i. use the function `optim`. It returns a list that among other things contains the four estimated parameters. You should set the following arguments:  
    `par`: you can set _c_ and _d_ as 1. Find good choices for _a_ and _b_ yourself (and argue why they are appropriate) 
    `fn`: which function to minimise?  
    `data`: the data frame with _x_, _target.frames_, and _y_, _correct_ in it  
    `method`: 'L-BFGS-B'  
    `lower`: lower bounds for the four parameters, (the lowest value they can take), you can set _c_ and _d_ as `-Inf`. Find good choices for _a_ and _b_ yourself (and argue why they are appropriate)  
    `upper`: upper bounds for the four parameters, (the highest value they can take) can set _c_ and _d_ as `Inf`. Find good choices for _a_ and _b_ yourself (and argue why they are appropriate)  
        
```{r -7.1}
subject7 <- df %>%
  dplyr::filter(subject == '7') %>% 
  dplyr::select('x' = target.frames, 'y' = correct, pas)


par1 <- optim(par = c(0.5, 1, 1, 1),
      fn = RSS, 
      data = filter(subject7, pas == '1'), 
      method = 'L-BFGS-B', 
      lower = c(0.5, 0.5, -Inf, -Inf), 
      upper = c(1, 1, Inf, Inf))

par2 <- optim(par = c(0.5, 1, 1, 1),
      fn = RSS, 
      data = filter(subject7, pas == '2'), 
      method = 'L-BFGS-B', 
      lower = c(0.5, 0.5, -Inf, -Inf), 
      upper = c(1, 1, Inf, Inf))

par3 <- optim(par = c(0.5, 1, 1, 1),
      fn = RSS, 
      data = filter(subject7, pas == '3'), 
      method = 'L-BFGS-B', 
      lower = c(0.5, 0.5, -Inf, -Inf), 
      upper = c(1, 1, Inf, Inf))

par4 <- optim(par = c(0.5, 1, 1, 1),
      fn = RSS, 
      data = filter(subject7, pas == '4'), 
      method = 'L-BFGS-B', 
      lower = c(0.5, 0.5, -Inf, -Inf), 
      upper = c(1, 1, Inf, Inf))


   
```

ii. Plot the fits for the PAS ratings on a single plot (for subject 7) xlim=c(0, 8)
```{r -7.1 ii}
 
newdf <- data.frame(cbind('x' = seq(0, 8, by = 0.01)))
#I've created a new data frame that with a whole lot of data points will create a line that 
newdf$yhat1 <- par1$par[1] + ((par1$par[2]-par1$par[1])/(1 + exp((par1$par[3]-newdf$x)/par1$par[4])))
newdf$yhat2 <- par2$par[1] + ((par2$par[2]-par2$par[1])/(1 + exp((par2$par[3]-newdf$x)/par2$par[4])))
newdf$yhat3 <- par3$par[1] + ((par3$par[2]-par3$par[1])/(1 + exp((par3$par[3]-newdf$x)/par3$par[4])))
newdf$yhat4 <- par4$par[1] + ((par4$par[2]-par4$par[1])/(1 + exp((par4$par[3]-newdf$x)/par4$par[4])))
#I've estimated yhat values for each of the parameters in the different passes 

ggplot(newdf) + #plotting
  geom_line(aes(x = x, y = yhat1, color = 'blue') ) + 
  geom_line(aes(x = x, y = yhat2, color = 'green')) + 
  geom_line(aes(x = x, y = yhat3, color = 'red')) + 
  geom_line(aes(x = x, y = yhat4, color = 'orange')) + 
  scale_color_discrete(name="Pas", 
                       breaks=c("blue", "green", "red", "orange"),
                       labels=c('Pas1', 'Pas2', 'Pas3', 'Pas4'))+
  xlim(c(0, 8)) +
  ylim(c(0, 1)) + 
  xlab('Target Frames') + 
  ylab('Predicted Accuracy') +
  theme_minimal()
```
    
  iii. Create a similar plot for the PAS ratings on a single plot (for subject 7), but this time based on the model from 6.1 `xlim=c(0, 8)`   
```{r -7.1 iii}
newdat <- data.frame(cbind('target.frames' = seq(0, 8, by = 0.001), 'pas' = rep(1:4), 'subject' = rep('7')))
#creating a data frame of fake combinations of target.frames and passes for subject 7 

newdat$subject <- as.factor(newdat$subject)
newdat$pas <- as.factor(newdat$pas)
newdat$target.frames <- as.numeric(newdat$target.frames)

newdat$yhat <- inv.logit(predict(interact, newdata = newdat))
#creates estimated y-values of 'correct' from our interaction model from 6.1. I inv-logit it to get it from the logarithm scale back to probability scale 

#plot it for each pas 
ggplot(newdat) + 
  geom_line(aes(x = target.frames, y = yhat, color = pas)) + 
  facet_wrap(~pas)+
  xlim(c(0, 8)) +
  ylim(c(0, 1)) + 
  xlab('Target Frames') + 
  ylab('Predicted Accuracy') +
  theme_minimal()
```
    iv. Comment on the differences between the fits - mention some advantages and disadvantages of each way  
???
The differences is seen especially on pas1 (the red lines) and pas 2 (the green lines). the Pas1 line indicates that the sigmoid plot is restricted to stay above 50% chance, while in the second plot it is allowed to go below chance and besides that it allows to increase. 

Besides this we only see a special difference in the pas2 (green line) from the first to the second plot. in the sigmoid 

2) Finally, estimate the parameters for all subjects and each of their four PAS ratings. 
```{r -7.2}
#creating a date frame to use for the data of the loop with the needed variables to run the optim function
subjectsdf <- df %>% 
  dplyr::select('x' = 'target.frames', 'y' = 'correct', 'pas', 'subject')

output <- data.frame()

#checking how long the subject variable is
length(unique(df$subject))
#it's 29, so I know the first part of the loop will 
       
#I'll make a loop which finds every parameter for each pas four ratings for each subject and the 
#in the first loop I only want it to go through all subjects
for (i in 1:29){
# in this inner loop I want it to go through all passes 1:4 because there are 4 passes
   for (n in 1:4) {
      #subset of data frame
    pas_df <- subjectsdf %>% 
      filter(subject == i & pas == n)
  
    parameters <- optim(
      par = c(0.5, 1, 1, 1),
      fn = RSS, 
      data = pas_df, 
      method = 'L-BFGS-B', 
      lower = c(0.5, 0.5, -Inf, -Inf), 
      upper = c(1, 1, Inf, Inf))
 
    better_output <- data.frame(
      subject=i, 
      pas=n,
      a = parameters$par[1],
      b = parameters$par[2],
      c = parameters$par[3],
      d = parameters$par[4])
    
    output <- rbind(output, better_output)
   }
}

#find mean of output

summarised_output <- output %>% 
  group_by(pas) %>% 
  summarise(mean.a=mean(a), mean.b=mean(b), mean.c=mean(c), mean.d=mean(d))
```

Then plot the estimated function at the group-level by taking the mean for each of the four parameters, _a_, _b_, _c_ and _d_ across subjects. A function should be estimated for each PAS-rating (it should look somewhat similar to Fig. 3 from the article:  https://doi.org/10.1016/j.concog.2019.03.007)
```{r -7.2}
plotdf <- data.frame(cbind('x' = seq(0, 8, by = 0.01)))

plotdf$yhat1 <- summarised_output$mean.a[1] + ((summarised_output$mean.b[1]-summarised_output$mean.a[1])/(1 + exp((summarised_output$mean.c[1]-plotdf$x)/summarised_output$mean.d[1])))
plotdf$yhat2 <- summarised_output$mean.a[2] + ((summarised_output$mean.b[2]-summarised_output$mean.a[2])/(1 + exp((summarised_output$mean.c[2]-plotdf$x)/summarised_output$mean.d[2])))
plotdf$yhat3 <- summarised_output$mean.a[3] + ((summarised_output$mean.b[3]-summarised_output$mean.a[3])/(1 + exp((summarised_output$mean.c[3]-plotdf$x)/summarised_output$mean.d[3])))
plotdf$yhat4 <- summarised_output$mean.a[4] + ((summarised_output$mean.b[4]-summarised_output$mean.a[4])/(1 + exp((summarised_output$mean.c[4]-plotdf$x)/summarised_output$mean.d[4])))

ggplot(plotdf) + 
  geom_line(aes(x = x, y = yhat1), color = 'red') + 
  geom_line(aes(x = x, y = yhat2), color = 'green') + 
  geom_line(aes(x = x, y = yhat3), color = 'blue') + 
  geom_line(aes(x = x, y = yhat4), color = 'purple') + 
  xlim(c(0, 8)) +
  ylim(c(0, 1)) + 
  xlab('Target Frames') + 
  ylab('Predicted Accuracy') +
  theme_minimal()
```

  i. compare with the figure you made in 5.3.ii and comment on the differences between the fits - mention some advantages and disadvantages of both.

???
The overall tendencies are the same. We see in both plots that the estimated accuracy based on pas e.g. for the pas1 rating, the estimated accuracy does not improve when the participants are shown more target frames

